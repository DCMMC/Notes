@incollection{word2vec,
	title = {Distributed Representations of Words and Phrases and their Compositionality},
	author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
	booktitle = {NeurIPS},
	editor = {C. J. C. Burges and L. Bottou and M. Welling and Z. Ghahramani and K. Q. Weinberger},
	pages = {3111--3119},
	year = {2013},
	url = {http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf}
}

@inproceedings{SIM,
	author = {Pi, Qi and Zhou, Guorui and Zhang, Yujing and Wang, Zhe and Ren, Lejian and Fan, Ying and Zhu, Xiaoqiang and Gai, Kun},
	title = {Search-Based User Interest Modeling with Lifelong Sequential Behavior Data for Click-Through Rate Prediction},
	year = {2020},
	isbn = {9781450368599},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3340531.3412744},
	doi = {10.1145/3340531.3412744},
	booktitle = {Proceedings of the 29th ACM International Conference on Information \& Knowledge Management},
	pages = {2685–2692},
	numpages = {8},
	keywords = {click-through rate prediction, long sequential user behavior data, user interest modeling},
	location = {Virtual Event, Ireland},
	series = {CIKM '20}
}

@inproceedings{lightgcn,
	author = {He, Xiangnan and Deng, Kuan and Wang, Xiang and Li, Yan and Zhang, YongDong and Wang, Meng},
	title = {LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation},
	year = {2020},
	isbn = {9781450380164},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3397271.3401063},
	doi = {10.1145/3397271.3401063},
	booktitle = {Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
	pages = {639–648},
	numpages = {10},
	keywords = {graph neural network, recommendation, collaborative filtering, embedding propagation},
	location = {Virtual Event, China},
	series = {SIGIR '20}
}

@inproceedings{NGCF,
	author = {Wang, Xiang and He, Xiangnan and Wang, Meng and Feng, Fuli and Chua, Tat-Seng},
	title = {Neural Graph Collaborative Filtering},
	year = {2019},
	isbn = {9781450361729},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3331184.3331267},
	doi = {10.1145/3331184.3331267},
	booktitle = {Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval},
	pages = {165–174},
	numpages = {10},
	keywords = {graph neural network, recommendation, collaborative filtering, high-order connectivity, embedding propagation},
	location = {Paris, France},
	series = {SIGIR'19}
}

@inproceedings{GCN,
	title={Semi-Supervised Classification with Graph Convolutional Networks},
	author={Kipf, Thomas N. and Welling, Max},
	booktitle={International Conference on Learning Representations (ICLR)},
	year={2017}
}

@inproceedings{GMCF,
	author = {Su, Yixin and Zhang, Rui and M. Erfani, Sarah and Gan, Junhao},
	title = {Neural Graph Matching Based Collaborative Filtering},
	year = {2021},
	isbn = {9781450380379},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3404835.3462833},
	doi = {10.1145/3404835.3462833},
	booktitle = {Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval},
	pages = {849–858},
	numpages = {10},
	keywords = {graph neural networks, neural graph matching, recommender systems, attribute interactions, collaborative filtering},
	location = {Virtual Event, Canada},
	series = {SIGIR '21}
}

@inproceedings{MB-GMN,
	author = {Xia, Lianghao and Xu, Yong and Huang, Chao and Dai, Peng and Bo, Liefeng},
	title = {Graph Meta Network for Multi-Behavior Recommendation},
	year = {2021},
	isbn = {9781450380379},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3404835.3462972},
	doi = {10.1145/3404835.3462972},
	booktitle = {Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval},
	pages = {757–766},
	numpages = {10},
	keywords = {multi-behavior recommendation, meta learning, graph neural networks},
	location = {Virtual Event, Canada},
	series = {SIGIR '21}
}

@inproceedings{UBR2CTR,
	author = {Qin, Jiarui and Zhang, Weinan and Wu, Xin and Jin, Jiarui and Fang, Yuchen and Yu, Yong},
	title = {User Behavior Retrieval for Click-Through Rate Prediction},
	year = {2020},
	isbn = {9781450380164},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3397271.3401440},
	doi = {10.1145/3397271.3401440},
	abstract = {Click-through rate (CTR) prediction plays a key role in modern online personalization services. In practice, it is necessary to capture user's drifting interests by modeling sequential user behaviors to build an accurate CTR prediction model. However, as the users accumulate more and more behavioral data on the platforms, it becomes non-trivial for the sequential models to make use of the whole behavior history of each user. First, directly feeding the long behavior sequence will make online inference time and system load infeasible. Second, there is much noise in such long histories to fail the sequential model learning. The current industrial solutions mainly truncate the sequences and just feed recent behaviors to the prediction model, which leads to a problem that sequential patterns such as periodicity or long-term dependency are not embedded in the recent several behaviors but in far back history. To tackle these issues, in this paper we consider it from the data perspective instead of just designing more sophisticated yet complicated models and propose User Behavior Retrieval for CTR prediction (UBR4CTR) framework. In UBR4CTR, the most relevant and appropriate user behaviors will be firstly retrieved from the entire user history sequence using a learnable search method. These retrieved behaviors are then fed into a deep model to make the final prediction instead of simply using the most recent ones. It is highly feasible to deploy UBR4CTR into industrial model pipeline with low cost. Experiments on three real-world large-scale datasets demonstrate the superiority and efficacy of our proposed framework and models.},
	booktitle = {Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
	pages = {2347–2356},
	numpages = {10},
	keywords = {information retrieval, CTR prediction, sequential user behavior modeling},
	location = {Virtual Event, China},
	series = {SIGIR '20}
}


@misc{HMG-CR,
	title={Hyper Meta-Path Contrastive Learning for Multi-Behavior Recommendation}, 
	author={Haoran Yang and Hongxu Chen and Lin Li and Philip S. Yu and Guandong Xu},
	year={2021},
	eprint={2109.02859},
	archivePrefix={arXiv},
	primaryClass={cs.IR}
}

@inproceedings{wang2020global,
	title={Global Context Enhanced Graph Neural Networks for Session-based Recommendation},
	author={Wang, Ziyang and Wei, Wei and Cong, Gao and Li, Xiao-Li and Mao, Xian-Ling and Qiu, Minghui},
	booktitle={Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
	pages={169--178},
	year={2020}
}

@inproceedings{dat-mdi,
	author = {Chen, Chen and Guo, Jie and Song, Bin},
	title = {Dual Attention Transfer in Session-Based Recommendation with Multi-Dimensional Integration},
	year = {2021},
	isbn = {9781450380379},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3404835.3462866},
	doi = {10.1145/3404835.3462866},
	booktitle = {Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval},
	pages = {869–878},
	numpages = {10},
	keywords = {session-based recommendation, cross domain recommendation, graph neural networks},
	location = {Virtual Event, Canada},
	series = {SIGIR '21}
}


@inproceedings{clark2020electra,
	title = {{ELECTRA}: Pre-training Text Encoders as Discriminators Rather Than Generators},
	author = {Kevin Clark and Minh-Thang Luong and Quoc V. Le and Christopher D. Manning},
	booktitle = {ICLR},
	year = {2020},
	url = {https://openreview.net/pdf?id=r1xMH1BtvB}
}

@inproceedings{SURGE,
	author = {Chang, Jianxin and Gao, Chen and Zheng, Yu and Hui, Yiqun and Niu, Yanan and Song, Yang and Jin, Depeng and Li, Yong},
	title = {Sequential Recommendation with Graph Neural Networks},
	year = {2021},
	isbn = {9781450380379},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3404835.3462968},
	doi = {10.1145/3404835.3462968},
	booktitle = {Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval},
	pages = {378–387},
	numpages = {10},
	keywords = {sequential recommendation, graph neural networks, dynamic user preferences},
	location = {Virtual Event, Canada},
	series = {SIGIR '21}
}

@inproceedings{DeCLUTR,
	title = "{D}e{CLUTR}: Deep Contrastive Learning for Unsupervised Textual Representations",
	author = "Giorgi, John  and
	Nitski, Osvald  and
	Wang, Bo  and
	Bader, Gary",
	booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
	month = aug,
	year = "2021",
	address = "Online",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2021.acl-long.72",
	doi = "10.18653/v1/2021.acl-long.72",
	pages = "879--895"
}

@misc{meng2021cocolm,
	title   = {COCO-LM: Correcting and Contrasting Text Sequences for Language Model Pretraining}, 
	author  = {Yu Meng and Chenyan Xiong and Payal Bajaj and Saurabh Tiwary and Paul Bennett and Jiawei Han and Xia Song},
	year    = {2021},
	eprint  = {2102.08473},
	archivePrefix = {arXiv},
	primaryClass = {cs.CL}
}

@InProceedings{ImgTransformer,
	title = 	 {Image Transformer},
	author =       {Parmar, Niki and Vaswani, Ashish and Uszkoreit, Jakob and Kaiser, Lukasz and Shazeer, Noam and Ku, Alexander and Tran, Dustin},
	booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
	pages = 	 {4055--4064},
	year = 	 {2018},
	editor = 	 {Dy, Jennifer and Krause, Andreas},
	volume = 	 {80},
	series = 	 {Proceedings of Machine Learning Research},
	month = 	 {10--15 Jul},
	publisher =    {PMLR},
	pdf = 	 {http://proceedings.mlr.press/v80/parmar18a/parmar18a.pdf},
	url = 	 {https://proceedings.mlr.press/v80/parmar18a.html},
	abstract = 	 {Image generation has been successfully cast as an autoregressive sequence generation or transformation problem. Recent work has shown that self-attention is an effective way of modeling textual sequences. In this work, we generalize a recently proposed model architecture based on self-attention, the Transformer, to a sequence modeling formulation of image generation with a tractable likelihood. By restricting the self-attention mechanism to attend to local neighborhoods we significantly increase the size of images the model can process in practice, despite maintaining significantly larger receptive fields per layer than typical convolutional neural networks. While conceptually simple, our generative models significantly outperform the current state of the art in image generation on ImageNet, improving the best published negative log-likelihood on ImageNet from 3.83 to 3.77. We also present results on image super-resolution with a large magnification ratio, applying an encoder-decoder configuration of our architecture. In a human evaluation study, we find that images generated by our super-resolution model fool human observers three times more often than the previous state of the art.}
}

@inproceedings{gehrmann-etal-2018-bottom,
	title = "Bottom-Up Abstractive Summarization",
	author = "Gehrmann, Sebastian  and
	Deng, Yuntian  and
	Rush, Alexander",
	booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
	month = oct # "-" # nov,
	year = "2018",
	address = "Brussels, Belgium",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/D18-1443",
	doi = "10.18653/v1/D18-1443",
	pages = "4098--4109",
	abstract = "Neural summarization produces outputs that are fluent and readable, but which can be poor at content selection, for instance often copying full sentences from the source document. This work explores the use of data-efficient content selectors to over-determine phrases in a source document that should be part of the summary. We use this selector as a bottom-up attention step to constrain the model to likely phrases. We show that this approach improves the ability to compress text, while still generating fluent summaries. This two-step process is both simpler and higher performing than other end-to-end content selection models, leading to significant improvements on ROUGE for both the CNN-DM and NYT corpus. Furthermore, the content selector can be trained with as little as 1,000 sentences making it easy to transfer a trained summarizer to a new domain.",
}

@inproceedings{lee-etal-2017-end,
	title = "End-to-end Neural Coreference Resolution",
	author = "Lee, Kenton  and
	He, Luheng  and
	Lewis, Mike  and
	Zettlemoyer, Luke",
	booktitle = "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
	month = sep,
	year = "2017",
	address = "Copenhagen, Denmark",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/D17-1018",
	doi = "10.18653/v1/D17-1018",
	pages = "188--197",
	abstract = "We introduce the first end-to-end coreference resolution model and show that it significantly outperforms all previous work without using a syntactic parser or hand-engineered mention detector. The key idea is to directly consider all spans in a document as potential mentions and learn distributions over possible antecedents for each. The model computes span embeddings that combine context-dependent boundary representations with a head-finding attention mechanism. It is trained to maximize the marginal likelihood of gold antecedent spans from coreference clusters and is factored to enable aggressive pruning of potential mentions. Experiments demonstrate state-of-the-art performance, with a gain of 1.5 F1 on the OntoNotes benchmark and by 3.1 F1 using a 5-model ensemble, despite the fact that this is the first approach to be successfully trained with no external resources.",
}

@InProceedings{MoCo,
	author = {He, Kaiming and Fan, Haoqi and Wu, Yuxin and Xie, Saining and Girshick, Ross},
	title = {Momentum Contrast for Unsupervised Visual Representation Learning},
	booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
	month = {June},
	year = {2020}
}

@misc{fang2020cert,
	title={CERT: Contrastive Self-supervised Learning for Language Understanding}, 
	author={Hongchao Fang and Sicheng Wang and Meng Zhou and Jiayuan Ding and Pengtao Xie},
	year={2020},
	eprint={2005.12766},
	archivePrefix={arXiv},
	primaryClass={cs.CL}
}

@inproceedings{gao2021simcse,
	title={{SimCSE}: Simple Contrastive Learning of Sentence Embeddings},
	author={Gao, Tianyu and Yao, Xingcheng and Chen, Danqi},
	booktitle={Empirical Methods in Natural Language Processing (EMNLP)},
	year={2021}
}

@inproceedings{wang2020hypersphere,
	title={Understanding Contrastive Representation Learning through Alignment and Uniformity on the Hypersphere},
	author={Wang, Tongzhou and Isola, Phillip},
	booktitle={International Conference on Machine Learning},
	organization={PMLR},
	pages={9929--9939},
	year={2020}
}


@InProceedings{SimCLR,
	title = 	 {A Simple Framework for Contrastive Learning of Visual Representations},
	author =       {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
	booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
	pages = 	 {1597--1607},
	year = 	 {2020},
	editor = 	 {III, Hal Daumé and Singh, Aarti},
	volume = 	 {119},
	series = 	 {Proceedings of Machine Learning Research},
	month = 	 {13--18 Jul},
	publisher =    {PMLR},
	pdf = 	 {http://proceedings.mlr.press/v119/chen20j/chen20j.pdf},
	url = 	 {https://proceedings.mlr.press/v119/chen20j.html},
	abstract = 	 {This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5% top-1 accuracy, which is a 7% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1% of the labels, we achieve 85.8% top-5 accuracy, outperforming AlexNet with 100X fewer labels.}
}


@inproceedings{
	paulus2018a,
	title={A Deep Reinforced Model for Abstractive Summarization},
	author={Romain Paulus and Caiming Xiong and Richard Socher},
	booktitle={International Conference on Learning Representations},
	year={2018},
	url={https://openreview.net/forum?id=HkAClQgA-},
}

@inproceedings{copysumm,
	author={Abigail See and Peter J. Liu and Christopher D. Manning},
	title={Get To The Point: Summarization with Pointer-Generator Networks},
	year={2017},
	cdate={1483228800000},
	pages={1073-1083},
	url={https://doi.org/10.18653/v1/P17-1099},
	booktitle={ACL (1)},
	crossref={conf/acl/2017-1}
}

@inproceedings{
	huang2018music,
	title={Music Transformer},
	author={Cheng-Zhi Anna Huang and Ashish Vaswani and Jakob Uszkoreit and Ian Simon and Curtis Hawthorne and Noam Shazeer and Andrew M. Dai and Matthew D. Hoffman and Monica Dinculescu and Douglas Eck},
	booktitle={International Conference on Learning Representations},
	year={2019},
	url={https://openreview.net/forum?id=rJe4ShAcF7},
}


@inproceedings{ELMo,
	title = "Deep Contextualized Word Representations",
	author = "Peters, Matthew E.  and
	Neumann, Mark  and
	Iyyer, Mohit  and
	Gardner, Matt  and
	Clark, Christopher  and
	Lee, Kenton  and
	Zettlemoyer, Luke",
	booktitle = "NAACL",
	month = jun,
	year = "2018",
	url = "https://aclanthology.org/N18-1202",
	doi = "10.18653/v1/N18-1202",
	pages = "2227--2237"
}

@inproceedings{Transformer,
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
	booktitle = {NeurIPS},
	pages = {},
	title = {Attention is All you Need},
	url = {https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
	volume = {30},
	year = {2017}
}



@inproceedings{CoVe,
	author = {McCann, Bryan and Bradbury, James and Xiong, Caiming and Socher, Richard},
	booktitle = {NeurIPS},
	editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
	pages = {},
	title = {Learned in Translation: Contextualized Word Vectors},
	url = {https://proceedings.neurips.cc/paper/2017/file/20c86a628232a67e7bd46f76fba7ce12-Paper.pdf},
	volume = {30},
	year = {2017}
}

@misc{BERT,
	title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}, 
	author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
	year={2019},
	eprint={1810.04805},
	archivePrefix={arXiv},
	primaryClass={cs.CL}
}

@inproceedings{TagLM,
	author={Matthew E. Peters and Waleed Ammar and Chandra Bhagavatula and Russell Power},
	title={Semi-supervised sequence tagging with bidirectional language models},
	year={2017},
	cdate={1483228800000},
	pages={1756-1765},
	url={https://doi.org/10.18653/v1/P17-1161},
	booktitle={ACL (1)},
	crossref={conf/acl/2017-1}
}

@inproceedings{CharCNN,
	author = {Kim, Yoon and Jernite, Yacine and Sontag, David and Rush, Alexander M.},
	title = {Character-Aware Neural Language Models},
	year = {2016},
	publisher = {AAAI Press},
	booktitle = {Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence},
	pages = {2741–2749},
	numpages = {9},
	location = {Phoenix, Arizona},
	series = {AAAI'16}
}

@misc{CharDecoderLSTM,
	title={Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models}, 
	author={Minh-Thang Luong and Christopher D. Manning},
	year={2016},
	eprint={1604.00788},
	archivePrefix={arXiv},
	primaryClass={cs.CL}
}

@inproceedings{morphology,
	title={Better word representations with recursive neural networks for morphology},
	author={Luong, Minh-Thang and Socher, Richard and Manning, Christopher D},
	booktitle={Proceedings of the Seventeenth Conference on Computational Natural Language Learning},
	pages={104--113},
	year={2013}
}

@misc{BiDAF,
	Author = {Minjoon Seo and Aniruddha Kembhavi and Ali Farhadi and Hannaneh Hajishirzi},
	Title = {Bidirectional Attention Flow for Machine Comprehension},
	Year = {2016},
	Eprint = {arXiv:1611.01603},
}

@inproceedings{kim-2014-convolutional,
	title = "Convolutional Neural Networks for Sentence Classification",
	author = "Kim, Yoon",
	booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})",
	month = oct,
	year = "2014",
	address = "Doha, Qatar",
	publisher = "Association for Computational Linguistics",
	url = "https://www.aclweb.org/anthology/D14-1181",
	doi = "10.3115/v1/D14-1181",
	pages = "1746--1751",
}

@inproceedings{VDCNN,
	title = "Very Deep Convolutional Networks for Text Classification",
	author = {Conneau, Alexis  and
	Schwenk, Holger  and
	Barrault, Lo{\"\i}c  and
	Lecun, Yann},
	booktitle = "Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 1, Long Papers",
	month = apr,
	year = "2017",
	address = "Valencia, Spain",
	publisher = "Association for Computational Linguistics",
	url = "https://www.aclweb.org/anthology/E17-1104",
	pages = "1107--1116",
	abstract = "The dominant approach for many NLP tasks are recurrent neural networks, in particular LSTMs, and convolutional neural networks. However, these architectures are rather shallow in comparison to the deep convolutional networks which have pushed the state-of-the-art in computer vision. We present a new architecture (VDCNN) for text processing which operates directly at the character level and uses only small convolutions and pooling operations. We are able to show that the performance of this model increases with the depth: using up to 29 convolutional layers, we report improvements over the state-of-the-art on several public text classification tasks. To the best of our knowledge, this is the first time that very deep convolutional nets have been applied to text processing.",
}

@inproceedings{SAR,
	title = "A Thorough Examination of the {CNN}/Daily Mail Reading Comprehension Task",
	author = "Chen, Danqi  and
	Bolton, Jason  and
	Manning, Christopher D.",
	booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
	month = aug,
	year = "2016",
	address = "Berlin, Germany",
	publisher = "Association for Computational Linguistics",
	url = "https://www.aclweb.org/anthology/P16-1223",
	doi = "10.18653/v1/P16-1223",
	pages = "2358--2367",
}

@inproceedings{DrQA,
	title = "Reading {W}ikipedia to Answer Open-Domain Questions",
	author = "Chen, Danqi  and
	Fisch, Adam  and
	Weston, Jason  and
	Bordes, Antoine",
	booktitle = "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
	month = jul,
	year = "2017",
	address = "Vancouver, Canada",
	publisher = "Association for Computational Linguistics",
	url = "https://www.aclweb.org/anthology/P17-1171",
	doi = "10.18653/v1/P17-1171",
	pages = "1870--1879",
	abstract = "This paper proposes to tackle open-domain question answering using Wikipedia as the unique knowledge source: the answer to any factoid question is a text span in a Wikipedia article. This task of machine reading at scale combines the challenges of document retrieval (finding the relevant articles) with that of machine comprehension of text (identifying the answer spans from those articles). Our approach combines a search component based on bigram hashing and TF-IDF matching with a multi-layer recurrent neural network model trained to detect answers in Wikipedia paragraphs. Our experiments on multiple existing QA datasets indicate that (1) both modules are highly competitive with respect to existing counterparts and (2) multitask learning using distant supervision on their combination is an effective complete system on this challenging task.",
}

@article{SMT,
	title = "The Mathematics of Statistical Machine Translation: Parameter Estimation",
	author = "Brown, Peter F.  and
	Della Pietra, Stephen A.  and
	Della Pietra, Vincent J.  and
	Mercer, Robert L.",
	journal = "Computational Linguistics",
	volume = "19",
	number = "2",
	year = "1993",
	url = "https://www.aclweb.org/anthology/J93-2003",
	pages = "263--311",
}

@article{rohde2005_hacks,
	title={An Improved Model of Semantic Similarity Based on Lexical Co-Occurrence},
	author={Rohde, Douglas LT and Gonnerman, Laura M and Plaut, David C},
	year={2005}
}

@inproceedings{GloVe,
	title = "{G}love: Global Vectors for Word Representation",
	author = "Pennington, Jeffrey  and
	Socher, Richard  and
	Manning, Christopher",
	booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})",
	month = oct,
	year = "2014",
	address = "Doha, Qatar",
	publisher = "Association for Computational Linguistics",
	url = "https://www.aclweb.org/anthology/D14-1162",
	doi = "10.3115/v1/D14-1162",
	pages = "1532--1543",
}

@inproceedings{huang-etal-2012-improving,
	title = "Improving Word Representations via Global Context and Multiple Word Prototypes",
	author = "Huang, Eric  and
	Socher, Richard  and
	Manning, Christopher  and
	Ng, Andrew",
	booktitle = "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
	month = jul,
	year = "2012",
	address = "Jeju Island, Korea",
	publisher = "Association for Computational Linguistics",
	url = "https://www.aclweb.org/anthology/P12-1092",
	pages = "873--882",
}

@article{TACL_word_senses,
	author = {Sanjeev Arora and Yuanzhi Li and Yingyu Liang and Tengyu Ma and Andrej Risteski},
	title = {Linear Algebraic Structure of Word Senses, with Applications to Polysemy},
	journal = {Transactions of the Association for Computational Linguistics},
	volume = {6},
	number = {0},
	year = {2018},
	issn = {2307-387X},	pages = {483--495},	url = {https://transacl.org/index.php/tacl/article/view/1346}
}

@inproceedings{NER_ICML,
	author = {Collobert, Ronan and Weston, Jason},
	title = {A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning},
	year = {2008},
	isbn = {9781605582054},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/1390156.1390177},
	doi = {10.1145/1390156.1390177},
	booktitle = {Proceedings of the 25th International Conference on Machine Learning},
	pages = {160–167},
	numpages = {8},
	location = {Helsinki, Finland},
	series = {ICML ’08}
}

@article{LSTM,
	author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
	title = {Long Short-Term Memory},
	journal = {Neural Computation},
	volume = {9},
	number = {8},
	pages = {1735-1780},
	year = {1997},
	doi = {10.1162/neco.1997.9.8.1735},
	
	URL = { 
	https://doi.org/10.1162/neco.1997.9.8.1735
	
	},
	eprint = { 
	https://doi.org/10.1162/neco.1997.9.8.1735
	
	}
	,
	abstract = { Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms. }
}

@inproceedings{GRU,
	title = "Learning Phrase Representations using {RNN} Encoder{--}Decoder for Statistical Machine Translation",
	author = {Cho, Kyunghyun  and
	van Merri{\"e}nboer, Bart  and
	Gulcehre, Caglar  and
	Bahdanau, Dzmitry  and
	Bougares, Fethi  and
	Schwenk, Holger  and
	Bengio, Yoshua},
	booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})",
	month = oct,
	year = "2014",
	address = "Doha, Qatar",
	publisher = "Association for Computational Linguistics",
	url = "https://www.aclweb.org/anthology/D14-1179",
	doi = "10.3115/v1/D14-1179",
	pages = "1724--1734",
}


@inproceedings{RNN_vanishing,
	author = {Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua},
	title = {On the Difficulty of Training Recurrent Neural Networks},
	year = {2013},
	publisher = {JMLR.org},
	booktitle = {Proceedings of the 30th International Conference on International Conference on Machine Learning - Volume 28},
	pages = {III–1310–III–1318},
	numpages = {9},
	location = {Atlanta, GA, USA},
	series = {ICML’13}
}

@inproceedings{BatchNorm,
	author = {Ioffe, Sergey and Szegedy, Christian},
	title = {Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},
	year = {2015},
	publisher = {JMLR.org},
	booktitle = {Proceedings of the 32nd International Conference on International Conference on Machine Learning - Volume 37},
	pages = {448–456},
	numpages = {9},
	location = {Lille, France},
	series = {ICML’15}
}

@INPROCEEDINGS{ResNet, 
	author={K. {He} and X. {Zhang} and S. {Ren} and J. {Sun}}, 
	booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
	title={Deep Residual Learning for Image Recognition}, 
	year={2016}, 
	volume={}, 
	number={}, 
	pages={770-778}, 
	keywords={image classification;learning (artificial intelligence);neural nets;object detection;COCO segmentation;ImageNet localization;ILSVRC & COCO 2015 competitions;deep residual nets;COCO object detection dataset;visual recognition tasks;CIFAR-10;ILSVRC 2015 classification task;ImageNet test set;VGG nets;residual nets;ImageNet dataset;residual function learning;deeper neural network training;image recognition;deep residual learning;Training;Degradation;Complexity theory;Image recognition;Neural networks;Visualization;Image segmentation}, 
	doi={10.1109/CVPR.2016.90}, 
	ISSN={1063-6919}, 
	month={June},}

@INPROCEEDINGS{DenseNet, 
	author={G. {Huang} and Z. {Liu} and L. v. d. {Maaten} and K. Q. {Weinberger}}, 
	booktitle={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
	title={Densely Connected Convolutional Networks}, 
	year={2017}, 
	volume={}, 
	number={}, 
	pages={2261-2269}, 
	keywords={convolution;feedforward neural nets;learning (artificial intelligence);DenseNet;traditional convolutional networks;feature propagation;feature reuse;object recognition benchmark tasks;dense convolutional network;vanishing-gradient problem;Training;Convolution;Network architecture;Convolutional codes;Neural networks;Road transportation}, 
	doi={10.1109/CVPR.2017.243}, 
	ISSN={1063-6919}, 
	month={July},}

@inproceedings{nivre-2003-efficient,
	title = "An Efficient Algorithm for Projective Dependency Parsing",
	author = "Nivre, Joakim",
	booktitle = "Proceedings of the Eighth International Conference on Parsing Technologies",
	month = apr,
	year = "2003",
	address = "Nancy, France",
	url = "https://www.aclweb.org/anthology/W03-3017",
	pages = "149--160",
	abstract = "This paper presents a deterministic parsing algorithm for projective dependency grammar. The running time of the algorithm is linear in the length of the input string, and the dependency graph produced is guaranteed to be projective and acyclic. The algorithm has been experimentally evaluated in parsing unrestricted Swedish text, achieving an accuracy above 85{\%} with a very simple grammar.",
}

@inproceedings{chen-manning-2014-fast,
	title = "A Fast and Accurate Dependency Parser using Neural Networks",
	author = "Chen, Danqi  and
	Manning, Christopher",
	booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})",
	month = oct,
	year = "2014",
	address = "Doha, Qatar",
	publisher = "Association for Computational Linguistics",
	url = "https://www.aclweb.org/anthology/D14-1082",
	doi = "10.3115/v1/D14-1082",
	pages = "740--750",
}

@article{dropout,
	author  = {Nitish Srivastava and Geoffrey Hinton and Alex Krizhevsky and Ilya Sutskever and Ruslan Salakhutdinov},
	title   = {Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
	journal = {Journal of Machine Learning Research},
	year    = {2014},
	volume  = {15},
	pages   = {1929-1958},
	url     = {http://jmlr.org/papers/v15/srivastava14a.html}
}

@article{Adam,
	title={Adam: A method for stochastic optimization},
	author={Kingma, Diederik P and Ba, Jimmy},
	journal={arXiv preprint arXiv:1412.6980},
	year={2014}
}

@InProceedings{Xavier,
	title = 	 {Understanding the difficulty of training deep feedforward neural networks},
	author = 	 {Xavier Glorot and Yoshua Bengio},
	booktitle = 	 {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},
	pages = 	 {249--256},
	year = 	 {2010},
	editor = 	 {Yee Whye Teh and Mike Titterington},
	volume = 	 {9},
	series = 	 {Proceedings of Machine Learning Research},
	address = 	 {Chia Laguna Resort, Sardinia, Italy},
	month = 	 {13--15 May},
	publisher = 	 {PMLR},
	pdf = 	 {http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf},
	url = 	 {http://proceedings.mlr.press/v9/glorot10a.html},
	abstract = 	 {Whereas before 2006 it appears that deep multi-layer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future.  We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1.  Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence.}
}

@Article{rosenblatt58perceptron,
  author =       {Frank Rosenblatt},
  title =        {The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain},
  journal =      {Psychological Review},
  year =         {1958},
  volume =    {65},
  pages =     {386--408},
  note =      {Reprinted in \emph{Neurocomputing} (MIT Press, 1998)}
}


@Book{mitchell97book,
  author =    {Tom M. Mitchell},
  title =        {Machine Learning},
  publisher =    {McGraw Hill},
  year =         {1997},
}


@InProceedings{brin95nn,
  author =       {Sergey Brin},
  title =        {Near neighbor search in large metric spaces},
  year =      {1995},
  booktitle = {Conference on Very Large Databases (VLDB)}
}



@article{bendavid,
  title={Analysis of representations for domain adaptation},
  author={Ben-David, Shai and Blitzer, John and Crammer, Koby and Pereira, Fernando},
  journal={Advances in neural information processing systems},
  volume={19},
  pages={137},
  year={2007},
}

@article{quinlan,
  title={Induction of decision trees},
  author={Quinlan, J. Ross},
  journal={Machine learning},
  volume={1},
  number={1},
  pages={81--106},
  year={1986},
  publisher={Springer}
}

@inproceedings{ross,
  title = {A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning},
  author = {St\'ephane Ross and Geoff J. Gordon and J. Andrew Bagnell},
  booktitle = "Proceedings of the Workshop on Artificial Intelligence and Statistics (AIStats)",
  year = {2011}
}

@Unpublished{kaariainen,
  author =       {Matti K{\"a}{\"a}ri{\"a}inen},
  title =        {Lower bounds for reductions},
  note =         {Talk at the Atomic Learning Workshop (TTI-C)},
  month =        {March},
  year =         {2006},
}

@inproceedings{bickel,
  author = {Steffen Bickel and Michael Bruckner and Tobias Scheffer},
  title = {Discriminative Learning for Differing Training and Test Distributions},
  year = {2007},
  booktitle = "Proceedings of the International Conference on Machine Learning (ICML)"
}

@InProceedings{daume,
  author =       {Hal {Daum\'e III}},
  title =        {Frustratingly Easy Domain Adaptation},
  booktitle =    {Conference of the Association for Computational Linguistics (ACL)},
  year =         {2007},
  address =      {Prague, Czech Republic},
}


@article{friedler,
  title={On the (im)possibility of fairness},
  author={Friedler, Sorelle A and Scheidegger, Carlos and Venkatasubramanian, Suresh},
  journal={arXiv preprint arXiv:1609.07236},
  year={2016}
}


@inproceedings{hardt,
  title={Equality of opportunity in supervised learning},
  author={Hardt, Moritz and Price, Eric and Srebro, Nathan},
  booktitle={Advances in Neural Information Processing Systems},
  pages={3315--3323},
  year={2016}
}
