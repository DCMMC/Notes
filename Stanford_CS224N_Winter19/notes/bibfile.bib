@incollection{word2vec,
	title = {Distributed Representations of Words and Phrases and their Compositionality},
	author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
	booktitle = {NeurIPS},
	editor = {C. J. C. Burges and L. Bottou and M. Welling and Z. Ghahramani and K. Q. Weinberger},
	pages = {3111--3119},
	year = {2013},
	url = {http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf}
}


@InProceedings{ImgTransformer,
	title = 	 {Image Transformer},
	author =       {Parmar, Niki and Vaswani, Ashish and Uszkoreit, Jakob and Kaiser, Lukasz and Shazeer, Noam and Ku, Alexander and Tran, Dustin},
	booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
	pages = 	 {4055--4064},
	year = 	 {2018},
	editor = 	 {Dy, Jennifer and Krause, Andreas},
	volume = 	 {80},
	series = 	 {Proceedings of Machine Learning Research},
	month = 	 {10--15 Jul},
	publisher =    {PMLR},
	pdf = 	 {http://proceedings.mlr.press/v80/parmar18a/parmar18a.pdf},
	url = 	 {https://proceedings.mlr.press/v80/parmar18a.html},
	abstract = 	 {Image generation has been successfully cast as an autoregressive sequence generation or transformation problem. Recent work has shown that self-attention is an effective way of modeling textual sequences. In this work, we generalize a recently proposed model architecture based on self-attention, the Transformer, to a sequence modeling formulation of image generation with a tractable likelihood. By restricting the self-attention mechanism to attend to local neighborhoods we significantly increase the size of images the model can process in practice, despite maintaining significantly larger receptive fields per layer than typical convolutional neural networks. While conceptually simple, our generative models significantly outperform the current state of the art in image generation on ImageNet, improving the best published negative log-likelihood on ImageNet from 3.83 to 3.77. We also present results on image super-resolution with a large magnification ratio, applying an encoder-decoder configuration of our architecture. In a human evaluation study, we find that images generated by our super-resolution model fool human observers three times more often than the previous state of the art.}
}

@inproceedings{
	huang2018music,
	title={Music Transformer},
	author={Cheng-Zhi Anna Huang and Ashish Vaswani and Jakob Uszkoreit and Ian Simon and Curtis Hawthorne and Noam Shazeer and Andrew M. Dai and Matthew D. Hoffman and Monica Dinculescu and Douglas Eck},
	booktitle={International Conference on Learning Representations},
	year={2019},
	url={https://openreview.net/forum?id=rJe4ShAcF7},
}


@inproceedings{ELMo,
	title = "Deep Contextualized Word Representations",
	author = "Peters, Matthew E.  and
	Neumann, Mark  and
	Iyyer, Mohit  and
	Gardner, Matt  and
	Clark, Christopher  and
	Lee, Kenton  and
	Zettlemoyer, Luke",
	booktitle = "NAACL",
	month = jun,
	year = "2018",
	url = "https://aclanthology.org/N18-1202",
	doi = "10.18653/v1/N18-1202",
	pages = "2227--2237"
}

@inproceedings{Transformer,
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
	booktitle = {NeurIPS},
	pages = {},
	title = {Attention is All you Need},
	url = {https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
	volume = {30},
	year = {2017}
}



@inproceedings{CoVe,
	author = {McCann, Bryan and Bradbury, James and Xiong, Caiming and Socher, Richard},
	booktitle = {NeurIPS},
	editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
	pages = {},
	title = {Learned in Translation: Contextualized Word Vectors},
	url = {https://proceedings.neurips.cc/paper/2017/file/20c86a628232a67e7bd46f76fba7ce12-Paper.pdf},
	volume = {30},
	year = {2017}
}

@misc{BERT,
	title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}, 
	author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
	year={2019},
	eprint={1810.04805},
	archivePrefix={arXiv},
	primaryClass={cs.CL}
}

@inproceedings{TagLM,
	author={Matthew E. Peters and Waleed Ammar and Chandra Bhagavatula and Russell Power},
	title={Semi-supervised sequence tagging with bidirectional language models},
	year={2017},
	cdate={1483228800000},
	pages={1756-1765},
	url={https://doi.org/10.18653/v1/P17-1161},
	booktitle={ACL (1)},
	crossref={conf/acl/2017-1}
}

@inproceedings{CharCNN,
	author = {Kim, Yoon and Jernite, Yacine and Sontag, David and Rush, Alexander M.},
	title = {Character-Aware Neural Language Models},
	year = {2016},
	publisher = {AAAI Press},
	booktitle = {Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence},
	pages = {2741â€“2749},
	numpages = {9},
	location = {Phoenix, Arizona},
	series = {AAAI'16}
}

@misc{CharDecoderLSTM,
	title={Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models}, 
	author={Minh-Thang Luong and Christopher D. Manning},
	year={2016},
	eprint={1604.00788},
	archivePrefix={arXiv},
	primaryClass={cs.CL}
}

@inproceedings{morphology,
	title={Better word representations with recursive neural networks for morphology},
	author={Luong, Minh-Thang and Socher, Richard and Manning, Christopher D},
	booktitle={Proceedings of the Seventeenth Conference on Computational Natural Language Learning},
	pages={104--113},
	year={2013}
}

@misc{BiDAF,
	Author = {Minjoon Seo and Aniruddha Kembhavi and Ali Farhadi and Hannaneh Hajishirzi},
	Title = {Bidirectional Attention Flow for Machine Comprehension},
	Year = {2016},
	Eprint = {arXiv:1611.01603},
}

@inproceedings{kim-2014-convolutional,
	title = "Convolutional Neural Networks for Sentence Classification",
	author = "Kim, Yoon",
	booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})",
	month = oct,
	year = "2014",
	address = "Doha, Qatar",
	publisher = "Association for Computational Linguistics",
	url = "https://www.aclweb.org/anthology/D14-1181",
	doi = "10.3115/v1/D14-1181",
	pages = "1746--1751",
}

@inproceedings{VDCNN,
	title = "Very Deep Convolutional Networks for Text Classification",
	author = {Conneau, Alexis  and
	Schwenk, Holger  and
	Barrault, Lo{\"\i}c  and
	Lecun, Yann},
	booktitle = "Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 1, Long Papers",
	month = apr,
	year = "2017",
	address = "Valencia, Spain",
	publisher = "Association for Computational Linguistics",
	url = "https://www.aclweb.org/anthology/E17-1104",
	pages = "1107--1116",
	abstract = "The dominant approach for many NLP tasks are recurrent neural networks, in particular LSTMs, and convolutional neural networks. However, these architectures are rather shallow in comparison to the deep convolutional networks which have pushed the state-of-the-art in computer vision. We present a new architecture (VDCNN) for text processing which operates directly at the character level and uses only small convolutions and pooling operations. We are able to show that the performance of this model increases with the depth: using up to 29 convolutional layers, we report improvements over the state-of-the-art on several public text classification tasks. To the best of our knowledge, this is the first time that very deep convolutional nets have been applied to text processing.",
}

@inproceedings{SAR,
	title = "A Thorough Examination of the {CNN}/Daily Mail Reading Comprehension Task",
	author = "Chen, Danqi  and
	Bolton, Jason  and
	Manning, Christopher D.",
	booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
	month = aug,
	year = "2016",
	address = "Berlin, Germany",
	publisher = "Association for Computational Linguistics",
	url = "https://www.aclweb.org/anthology/P16-1223",
	doi = "10.18653/v1/P16-1223",
	pages = "2358--2367",
}

@inproceedings{DrQA,
	title = "Reading {W}ikipedia to Answer Open-Domain Questions",
	author = "Chen, Danqi  and
	Fisch, Adam  and
	Weston, Jason  and
	Bordes, Antoine",
	booktitle = "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
	month = jul,
	year = "2017",
	address = "Vancouver, Canada",
	publisher = "Association for Computational Linguistics",
	url = "https://www.aclweb.org/anthology/P17-1171",
	doi = "10.18653/v1/P17-1171",
	pages = "1870--1879",
	abstract = "This paper proposes to tackle open-domain question answering using Wikipedia as the unique knowledge source: the answer to any factoid question is a text span in a Wikipedia article. This task of machine reading at scale combines the challenges of document retrieval (finding the relevant articles) with that of machine comprehension of text (identifying the answer spans from those articles). Our approach combines a search component based on bigram hashing and TF-IDF matching with a multi-layer recurrent neural network model trained to detect answers in Wikipedia paragraphs. Our experiments on multiple existing QA datasets indicate that (1) both modules are highly competitive with respect to existing counterparts and (2) multitask learning using distant supervision on their combination is an effective complete system on this challenging task.",
}

@article{SMT,
	title = "The Mathematics of Statistical Machine Translation: Parameter Estimation",
	author = "Brown, Peter F.  and
	Della Pietra, Stephen A.  and
	Della Pietra, Vincent J.  and
	Mercer, Robert L.",
	journal = "Computational Linguistics",
	volume = "19",
	number = "2",
	year = "1993",
	url = "https://www.aclweb.org/anthology/J93-2003",
	pages = "263--311",
}

@article{rohde2005_hacks,
	title={An Improved Model of Semantic Similarity Based on Lexical Co-Occurrence},
	author={Rohde, Douglas LT and Gonnerman, Laura M and Plaut, David C},
	year={2005}
}

@inproceedings{GloVe,
	title = "{G}love: Global Vectors for Word Representation",
	author = "Pennington, Jeffrey  and
	Socher, Richard  and
	Manning, Christopher",
	booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})",
	month = oct,
	year = "2014",
	address = "Doha, Qatar",
	publisher = "Association for Computational Linguistics",
	url = "https://www.aclweb.org/anthology/D14-1162",
	doi = "10.3115/v1/D14-1162",
	pages = "1532--1543",
}

@inproceedings{huang-etal-2012-improving,
	title = "Improving Word Representations via Global Context and Multiple Word Prototypes",
	author = "Huang, Eric  and
	Socher, Richard  and
	Manning, Christopher  and
	Ng, Andrew",
	booktitle = "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
	month = jul,
	year = "2012",
	address = "Jeju Island, Korea",
	publisher = "Association for Computational Linguistics",
	url = "https://www.aclweb.org/anthology/P12-1092",
	pages = "873--882",
}

@article{TACL_word_senses,
	author = {Sanjeev Arora and Yuanzhi Li and Yingyu Liang and Tengyu Ma and Andrej Risteski},
	title = {Linear Algebraic Structure of Word Senses, with Applications to Polysemy},
	journal = {Transactions of the Association for Computational Linguistics},
	volume = {6},
	number = {0},
	year = {2018},
	issn = {2307-387X},	pages = {483--495},	url = {https://transacl.org/index.php/tacl/article/view/1346}
}

@inproceedings{NER_ICML,
	author = {Collobert, Ronan and Weston, Jason},
	title = {A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning},
	year = {2008},
	isbn = {9781605582054},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/1390156.1390177},
	doi = {10.1145/1390156.1390177},
	booktitle = {Proceedings of the 25th International Conference on Machine Learning},
	pages = {160â€“167},
	numpages = {8},
	location = {Helsinki, Finland},
	series = {ICML â€™08}
}

@article{LSTM,
	author = {Hochreiter, Sepp and Schmidhuber, JÃ¼rgen},
	title = {Long Short-Term Memory},
	journal = {Neural Computation},
	volume = {9},
	number = {8},
	pages = {1735-1780},
	year = {1997},
	doi = {10.1162/neco.1997.9.8.1735},
	
	URL = { 
	https://doi.org/10.1162/neco.1997.9.8.1735
	
	},
	eprint = { 
	https://doi.org/10.1162/neco.1997.9.8.1735
	
	}
	,
	abstract = { Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms. }
}

@inproceedings{GRU,
	title = "Learning Phrase Representations using {RNN} Encoder{--}Decoder for Statistical Machine Translation",
	author = {Cho, Kyunghyun  and
	van Merri{\"e}nboer, Bart  and
	Gulcehre, Caglar  and
	Bahdanau, Dzmitry  and
	Bougares, Fethi  and
	Schwenk, Holger  and
	Bengio, Yoshua},
	booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})",
	month = oct,
	year = "2014",
	address = "Doha, Qatar",
	publisher = "Association for Computational Linguistics",
	url = "https://www.aclweb.org/anthology/D14-1179",
	doi = "10.3115/v1/D14-1179",
	pages = "1724--1734",
}


@inproceedings{RNN_vanishing,
	author = {Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua},
	title = {On the Difficulty of Training Recurrent Neural Networks},
	year = {2013},
	publisher = {JMLR.org},
	booktitle = {Proceedings of the 30th International Conference on International Conference on Machine Learning - Volume 28},
	pages = {IIIâ€“1310â€“IIIâ€“1318},
	numpages = {9},
	location = {Atlanta, GA, USA},
	series = {ICMLâ€™13}
}

@inproceedings{BatchNorm,
	author = {Ioffe, Sergey and Szegedy, Christian},
	title = {Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},
	year = {2015},
	publisher = {JMLR.org},
	booktitle = {Proceedings of the 32nd International Conference on International Conference on Machine Learning - Volume 37},
	pages = {448â€“456},
	numpages = {9},
	location = {Lille, France},
	series = {ICMLâ€™15}
}

@INPROCEEDINGS{ResNet, 
	author={K. {He} and X. {Zhang} and S. {Ren} and J. {Sun}}, 
	booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
	title={Deep Residual Learning for Image Recognition}, 
	year={2016}, 
	volume={}, 
	number={}, 
	pages={770-778}, 
	keywords={image classification;learning (artificial intelligence);neural nets;object detection;COCO segmentation;ImageNet localization;ILSVRC & COCO 2015 competitions;deep residual nets;COCO object detection dataset;visual recognition tasks;CIFAR-10;ILSVRC 2015 classification task;ImageNet test set;VGG nets;residual nets;ImageNet dataset;residual function learning;deeper neural network training;image recognition;deep residual learning;Training;Degradation;Complexity theory;Image recognition;Neural networks;Visualization;Image segmentation}, 
	doi={10.1109/CVPR.2016.90}, 
	ISSN={1063-6919}, 
	month={June},}

@INPROCEEDINGS{DenseNet, 
	author={G. {Huang} and Z. {Liu} and L. v. d. {Maaten} and K. Q. {Weinberger}}, 
	booktitle={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
	title={Densely Connected Convolutional Networks}, 
	year={2017}, 
	volume={}, 
	number={}, 
	pages={2261-2269}, 
	keywords={convolution;feedforward neural nets;learning (artificial intelligence);DenseNet;traditional convolutional networks;feature propagation;feature reuse;object recognition benchmark tasks;dense convolutional network;vanishing-gradient problem;Training;Convolution;Network architecture;Convolutional codes;Neural networks;Road transportation}, 
	doi={10.1109/CVPR.2017.243}, 
	ISSN={1063-6919}, 
	month={July},}

@inproceedings{nivre-2003-efficient,
	title = "An Efficient Algorithm for Projective Dependency Parsing",
	author = "Nivre, Joakim",
	booktitle = "Proceedings of the Eighth International Conference on Parsing Technologies",
	month = apr,
	year = "2003",
	address = "Nancy, France",
	url = "https://www.aclweb.org/anthology/W03-3017",
	pages = "149--160",
	abstract = "This paper presents a deterministic parsing algorithm for projective dependency grammar. The running time of the algorithm is linear in the length of the input string, and the dependency graph produced is guaranteed to be projective and acyclic. The algorithm has been experimentally evaluated in parsing unrestricted Swedish text, achieving an accuracy above 85{\%} with a very simple grammar.",
}

@inproceedings{chen-manning-2014-fast,
	title = "A Fast and Accurate Dependency Parser using Neural Networks",
	author = "Chen, Danqi  and
	Manning, Christopher",
	booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})",
	month = oct,
	year = "2014",
	address = "Doha, Qatar",
	publisher = "Association for Computational Linguistics",
	url = "https://www.aclweb.org/anthology/D14-1082",
	doi = "10.3115/v1/D14-1082",
	pages = "740--750",
}

@article{dropout,
	author  = {Nitish Srivastava and Geoffrey Hinton and Alex Krizhevsky and Ilya Sutskever and Ruslan Salakhutdinov},
	title   = {Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
	journal = {Journal of Machine Learning Research},
	year    = {2014},
	volume  = {15},
	pages   = {1929-1958},
	url     = {http://jmlr.org/papers/v15/srivastava14a.html}
}

@article{Adam,
	title={Adam: A method for stochastic optimization},
	author={Kingma, Diederik P and Ba, Jimmy},
	journal={arXiv preprint arXiv:1412.6980},
	year={2014}
}

@InProceedings{Xavier,
	title = 	 {Understanding the difficulty of training deep feedforward neural networks},
	author = 	 {Xavier Glorot and Yoshua Bengio},
	booktitle = 	 {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},
	pages = 	 {249--256},
	year = 	 {2010},
	editor = 	 {Yee Whye Teh and Mike Titterington},
	volume = 	 {9},
	series = 	 {Proceedings of Machine Learning Research},
	address = 	 {Chia Laguna Resort, Sardinia, Italy},
	month = 	 {13--15 May},
	publisher = 	 {PMLR},
	pdf = 	 {http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf},
	url = 	 {http://proceedings.mlr.press/v9/glorot10a.html},
	abstract = 	 {Whereas before 2006 it appears that deep multi-layer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future.  We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1.  Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence.}
}

@Article{rosenblatt58perceptron,
  author =       {Frank Rosenblatt},
  title =        {The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain},
  journal =      {Psychological Review},
  year =         {1958},
  volume =    {65},
  pages =     {386--408},
  note =      {Reprinted in \emph{Neurocomputing} (MIT Press, 1998)}
}


@Book{mitchell97book,
  author =    {Tom M. Mitchell},
  title =        {Machine Learning},
  publisher =    {McGraw Hill},
  year =         {1997},
}


@InProceedings{brin95nn,
  author =       {Sergey Brin},
  title =        {Near neighbor search in large metric spaces},
  year =      {1995},
  booktitle = {Conference on Very Large Databases (VLDB)}
}



@article{bendavid,
  title={Analysis of representations for domain adaptation},
  author={Ben-David, Shai and Blitzer, John and Crammer, Koby and Pereira, Fernando},
  journal={Advances in neural information processing systems},
  volume={19},
  pages={137},
  year={2007},
}

@article{quinlan,
  title={Induction of decision trees},
  author={Quinlan, J. Ross},
  journal={Machine learning},
  volume={1},
  number={1},
  pages={81--106},
  year={1986},
  publisher={Springer}
}

@inproceedings{ross,
  title = {A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning},
  author = {St\'ephane Ross and Geoff J. Gordon and J. Andrew Bagnell},
  booktitle = "Proceedings of the Workshop on Artificial Intelligence and Statistics (AIStats)",
  year = {2011}
}

@Unpublished{kaariainen,
  author =       {Matti K{\"a}{\"a}ri{\"a}inen},
  title =        {Lower bounds for reductions},
  note =         {Talk at the Atomic Learning Workshop (TTI-C)},
  month =        {March},
  year =         {2006},
}

@inproceedings{bickel,
  author = {Steffen Bickel and Michael Bruckner and Tobias Scheffer},
  title = {Discriminative Learning for Differing Training and Test Distributions},
  year = {2007},
  booktitle = "Proceedings of the International Conference on Machine Learning (ICML)"
}

@InProceedings{daume,
  author =       {Hal {Daum\'e III}},
  title =        {Frustratingly Easy Domain Adaptation},
  booktitle =    {Conference of the Association for Computational Linguistics (ACL)},
  year =         {2007},
  address =      {Prague, Czech Republic},
}


@article{friedler,
  title={On the (im)possibility of fairness},
  author={Friedler, Sorelle A and Scheidegger, Carlos and Venkatasubramanian, Suresh},
  journal={arXiv preprint arXiv:1609.07236},
  year={2016}
}


@inproceedings{hardt,
  title={Equality of opportunity in supervised learning},
  author={Hardt, Moritz and Price, Eric and Srebro, Nathan},
  booktitle={Advances in Neural Information Processing Systems},
  pages={3315--3323},
  year={2016}
}
